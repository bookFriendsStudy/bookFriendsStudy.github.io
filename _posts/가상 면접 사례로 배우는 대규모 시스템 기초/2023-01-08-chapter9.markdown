---
layout: post
title: 9. Web Crawler 설계
date: 2023-01-08 21:00:00 +0900
categories:
- 가상 면접 사례로 배우는 대규모 시스템 기초
author: hojunee
---
# 09. Web Crawler 설계

*오른쪽 위 … → 전체 너비 설정을 권장합니다*

## 웹 크롤러(Web Crawler)

- Web에서 새로 올라오거나 갱신된 콘텐츠를 찾아내는 로봇
- 링크를 따라가면서 새로운 콘텐츠를 수집

사용되는 분야

- **Search Engine Indexing**
    - 검색 엔진을 위한 Local Index 생성
- **Web Archiving**
    - 나중에 참조할 목적으로 웹 정보를 보관
- **Web Mining**
    - 인터넷으로부터 유용한 지식을 도출
- **Web Monitoring**
    - 저작권 / 상표권이 침해되는 사례 모니터링

→ **웹 크롤러가 처리해야 하는 데이터의 규모에 따라 이것의 복잡도가 달라진다.**

## 1. 문제 이해 및 설계 범위 확정

### Disclaimer:

- 엄청난 규모 확장성을 갖는 웹 크롤러를 설계하는 것은 엄청나게 어려운 일임!
    
    → 설계를 진행하기 전에 질문을 던져서 요구사항을 알아내고 설계범위를 좁히자.
    

### Questions:

- 크롤러의 주된 용도는 무엇인가?
- 매달 얼마나 많은 웹 페이지를 수집해야 하는가?
- 새로 만들어진 웹 페이지나 수정된 웹 페이지도 고려해야 하는가?
- 수집한 웹 페이지는 저장해야 하는가?
- 중복된 컨텐츠는 어떻게 처리해야 하는가?

### Criteria:

- **규모 확장성** → 거대한 웹을 대응할 수 있도록 병행성 등을 활용하기
- **안정성** → 비정상적인 입력이나 환경에 잘 대응하기
- **예절** → 짧은 시간 동안 너무 많은 요청을 보내지 않기
- **확장성** → 새로운 형태의 콘텐츠(이미지, …)를 지원하기 쉬워야한다.

### Estimation:

- 매달 10억개의 웹 페이지를 다운로드
- Query Per Second(QPS)
    - 1,000,000,000 / (30 * 24 * 3600) ~= 400 page
    - Peak QPS = 2 * QPS = 900
- Web Page의 크기 평균은 500k라고 가정
    - 10억 Page * 500k = 500TB / month
    - 5년간 보관해야 할 저장공간은 500TB * 12m * 5y = 30PB정도 필요할 것

---

## 2. 개략적 설계안 제시 및 동의 구하기

- 시작 URL 집합
    - 웹 크롤러가 크롤링을 시작하는 출발점
    - 만약 대학 웹사이트와 관련된 정보를 찾는다면?
        - `*.ac.kr`, `*.edu` 등의 도메인을 사용
    - 만약 전체 웹을 크롤링해야 한다면?
        - 시작 URL을 잘 골라야한다 → 크롤러가 가능한 많은 링크를 탐색할 수 있도록
        - Locality: `en-US`, `ko-KR`, …
        - Topic: 스포츠, 쇼핑, 건강
    - 정답은 없으므로 의도 → 답변 흐름을 자연스럽게 가져가자.
- 미수집 URL 저장소(URL Frontier)
    - **다운로드할 URL** ↔ 다운로드된 URL
    - FIFO Queue 스타일?
- 도메인 이름 변환기
    - URL → IP 주소로 변환
- HTML 다운로더(HTML Downloader)
    - URL Frontier→ 인터넷에서 웹 페이지를 다운로드하는 컴포넌트
- 콘텐츠 파서
    - 웹 페이지의 파싱(Parsing)과 검증(Validation)
    - 속도 이슈로 인해 크롤링 서버와 분리
- 중복 콘텐츠 체크
    - 웹의 29% 가량의 컨텐츠가 중복이라는 연구 결과가 있음
    - Approach 1: 두 HTML 문서를 비교
    - Approach 2: 웹 페이지의 Hash값을 비교
- 콘텐츠 저장소
    - HTML 문서를 보관
    - 고려해야 할 것들
        - 저장할 데이터의 유형
        - 크기
        - 저장소의 접근 빈도
        - 데이터의 유효 기간
        - …
    - 본 설계에서는 디스크와 메모리를 동시에 사용
        - 자주 쓰이는 데이터는 메모리에 두어 지연시간 Down
        - 데이터의 양이 너무 많으므로 이외에는 디스크에 저장
- URL 추출기
    - HTML 페이지를 파싱해서 (하이퍼)링크들을 골라낸다
        - 상대경로라면? → 절대경로로 변환
    - Example:
    
- URL 필터
    
    다음 URL들을 사전에 필터링하는 역할
    
    - 특정 콘텐츠 타입, 파일 확장자를 갖는 URL(e.g. `*.pdf`)
    - 접속시 오류가 발생하는 URL
    - Deny List에 포함된 URL
- URL 중복 체크
    
    이미 방문한 적 있는 URL인지를 체크 → 같은 URL을 여러 번 처리하는 일 방지
    
    - Bloom Filter
    - Hash Table
- URL 저장소
    
    이미 방문한 URL을 보관하는 저장소
    

---

## 3. 상세 설계

### DFS vs BFS

- 웹은 유향 그래프(Directed Graph)의 형태를 띤다.
    - Node: 페이지
    - Edge: 하이퍼링크
- DFS는 그래프 크기가 클 경우, 어느 정도로 깊숙히 가게 될지 가능하기 어려움
    - 따라서 **BFS**를 사용하는 것이 좋다.
- BFS의 문제점?
    - 지속적으로 같은 서버를 참조하는 문제 (→ **예절**에 위배)
    - URL 간의 우선순위를 두지 않음

### 미수집 URL 저장소

- 단시간에 너무 많은 요청을 보내는 것은 Impolite한 행위이며, DoS(Denial-of-Service) 공격으로 간주될 수 있음.

- **동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청하자**
    - 호스트명(hostname) ↔ 작업 스레드(worker thread)의 관계를 형성
    - 각 다운로드 스레드는 별도의 FIFO 큐를 가지고, 해당 큐에서 꺼낸 URL만 다운로드하자

- **큐 라우터**는 같은 호스트에 속한 URL이 언제나 같은 큐에 가도록 보장
- **큐 선택기**가 큐들을 순회하면서, 해당 큐에서 나온 URL을 다운로드하도록 지정된 작업 스레드에 전달
- **작업 스레드**는 전달된 URL을 다운로드하는 작업을 수행.

**우선 순위**

- 페이지랭크(PageRank), 트래픽 양, 갱신 빈도 등 다양한 척도를 사용
- **순위결정장치(Prioritizer)**

위의 두 Queue 시스템을 병합해서 다음과 같은 최종 시스템을 만들 수 있다.

- 전면 큐: 우선순위 결정 과정을 처리
- 후면 큐: 크롤러가 예의바르게 동작하도록 보증

**신선도**

- 웹 페이지는 수시로 추가되고, 삭제되고, 변경됨.
- 이미 다운로드 된 페이지라고 하더라도 주기적으로 재수집해줄 필요가 있음
- 어떻게 하면 이를 최적화할 수 있을까?
    - 웹 페이지의 변경 이력(Update History) 활용
    - 우선순위가 높은 페이지는 조금 더 자주 재수집

### HTML 다운로더

**Robot.txt**

- 크롤러가 수집해도 되는 페이지 목록이 들어있는 파일
- 크롤러는 해당 파일에 나열된 규칙을 먼저 확인해야 함

**성능 최적화**

- **분산 크롤링**
    - 크롤링 작업을 여러 서버에 분산
    

**DNS Resolver 캐싱**

- DNS Resolver는 크롤러 성능의 병목 중 하나임
    - DNS 요청을 보내고 받는 과정은 동기적인 성격을 띠기 때문.
    - e.g. 한 크롤러 스레드가 DNS 작업을 진행한다면 다른 스레드의 DNS 요청은 모두 블록됨
- DNS 조회 결과로 얻어진 도메인 이름과 IP 주소 사이의 관계를 캐싱해두자.

**지역성**

- 크롤링 대상 서버와 지역적으로 가까운 크롤링 서버를 배치

**짧은 타임아웃**

- 가망이 없는 서버에 대한 요청을 중단하고 다음 페이지로 넘어가는 전략

**안정성**

- 안정 해시
    - 다운로더 서버들에게 부하를 분산할 때 적용
    - 다운로더 서버를 쉽게 추가하고 삭제
- 크롤링 상태 및 수집 데이터 저장
    - 크롤링 상태와 데이터를 지속적으로 저장, 오류가 발생하더라도 중단 지점부터 쉽게 재시작 가능
- 예외 처리
- 데이터 검증

**확장성**

**문제 있는 콘텐츠 감지 및 회피**

- 중복 컨텐츠
    - 해시 등을 활용해서 중복 콘텐츠를 탐지, 재수집을 막는다
- 거미덫
    - ~~악질 중의 악질~~
    - 크롤러를 무한 루프로 빠뜨리는 웹 페이지 → URL 최대 길이 제한?
- 데이터 노이즈
    - 광고, 스팸성 페이지

---

## 4. Action Items:

- 서버 측 렌더링 (Server-Side Rendering)
    - JavaScript, Ajax 등의 기술을 사용해서 만들어낸 동적 페이지들을 어떻게 처리할 것인지?
- 원치 않는 페이지 필터링
    - 스팸 방지 컴포넌트를 추가할 수도 있음
- 데이터베이스 다중화 & 샤딩
    - 데이터 계층의 가용성, 규모 확장성, 안정성 향상
- 수평적 규모 확장성
    - 대규모 크롤링을 위해서는 다운로드를 실행할 서버가 매우 많아질수도
    - 이를 위해선 Stateless함을 유지할 필요가 있음.
- 가용성, 일관성, 안정성
    - 이 책의 이름(대규모 시스템 설계)를 리마인드
- 데이터 분석 솔루션
    - 데이터 수집에 그치지 말고 이를 활용해서 인사이트를 유도